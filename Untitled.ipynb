{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Wordnet Lemmatizer (only nouns)\n",
      "game\n",
      "gaming\n",
      "gamed\n",
      "game\n",
      "engineering\n",
      "engineer\n",
      "resume\n",
      "resumed\n",
      "resuming\n",
      "Output of Wordnet Lemmatizer (for verbs)\n",
      "game\n",
      "game\n",
      "game\n",
      "game\n",
      "engineer\n",
      "engineer\n",
      "resume\n",
      "resume\n",
      "resume\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wnl = WordNetLemmatizer() \n",
    "words = [\"game\",\"gaming\",\"gamed\",\"games\", 'engineering', 'engineers', 'resumes', 'resumed', 'resuming'] \n",
    "print ('Output of Wordnet Lemmatizer (only nouns)')\n",
    "for word in words: \n",
    "    print(wnl.lemmatize(word)) \n",
    "print ('Output of Wordnet Lemmatizer (for verbs)')\n",
    "for word in words: \n",
    "    print(wnl.lemmatize(word, pos='v')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Barack/NNP)\n",
      "(ORGANIZATION Obama/NNP)\n",
      "('meets', 'VBZ')\n",
      "(PERSON Michael/NNP Jackson/NNP)\n",
      "('in', 'IN')\n",
      "(GPE Nihonbashi/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, ne_chunk, pos_tag \n",
    "sentence = 'Barack Obama meets Michael Jackson in Nihonbashi' \n",
    "token = word_tokenize(sentence ) \n",
    "postag = pos_tag(token) \n",
    "chunked = ne_chunk(postag) \n",
    "for i in chunked: \n",
    "    print (i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sentence: ', 'I am the king of the world')\n",
      "('I', '/', 'PRP')\n",
      "('am', '/', 'VBP')\n",
      "('the', '/', 'DT')\n",
      "('king', '/', 'NN')\n",
      "('of', '/', 'IN')\n",
      "('the', '/', 'DT')\n",
      "('world', '/', 'NN')\n",
      "['king', 'world']\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "line = 'I am the king of the world' \n",
    "print ('Sentence: ', line) \n",
    "sentences = nltk.sent_tokenize(line) #tokenize sentences \n",
    "nouns = [] #empty to hold all nouns \n",
    "for sentence in sentences: \n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))): \n",
    "        print (word, '/', pos) \n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))): \n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):  # to extract only nouns \n",
    "             nouns.append(word) \n",
    "print (nouns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sentence: ', ['describe', 'your', 'work'])\n",
      "\n",
      "Parsing output\n",
      "[Tree('S', [Tree('V', ['describe']), Tree('NP', [Tree('PRP', ['your']), Tree('N', ['work'])])])]\n"
     ]
    }
   ],
   "source": [
    "your_grammar = nltk.CFG.fromstring(\"\"\" \n",
    "S -> V NP \n",
    "V -> 'describe' | 'present' \n",
    "NP -> PRP N  \n",
    "PRP -> 'your'  \n",
    "N -> 'work' \n",
    "\"\"\") \n",
    "parser = nltk.ChartParser(your_grammar) \n",
    "sent = 'describe your work'.split() \n",
    "print ('Sentence: ', sent )\n",
    "print ('') \n",
    "print ('Parsing output')\n",
    "print (list(parser.parse(sent))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('hit.v.01')\n",
      "Jiang-Conrath Similarity\n",
      "('hit & slap: ', 1e-300)\n",
      "('dog & cat: ', 0.28539390848096946)\n",
      "\n",
      "Resnik Similarity\n",
      "('hit & slap: ', 0)\n",
      "('dog & cat: ', 7.204023991374844)\n",
      "\n",
      "\n",
      "Lin Similarity using Genesis IC\n",
      "('hit & slap: ', 0.0)\n",
      "('dog & cat: ', 0.8043806652422295)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "from nltk.corpus import genesis \n",
    "#from nltk.corpus import brown \n",
    "genesis_ic = wn.ic(genesis, False, 0.0) \n",
    "#brown_ic = wn.ic(brown, False, 0.0) \n",
    "hit = wn.synset('hit.v.01')\n",
    "print(hit)\n",
    "slap = wn.synset('slap.v.01') \n",
    "dog = wn.synset('dog.n.01') \n",
    "cat = wn.synset('cat.n.01') \n",
    "print ('Jiang-Conrath Similarity' )\n",
    "print ('hit & slap: ', hit.jcn_similarity(slap, genesis_ic))     # Jiang-Conrath Similarity \n",
    "print ('dog & cat: ', dog.jcn_similarity(cat, genesis_ic) )\n",
    "print ('' )\n",
    "print ('Resnik Similarity' )\n",
    "print ('hit & slap: ', hit.res_similarity(slap, genesis_ic))     # Resnik Similarity \n",
    "print ('dog & cat: ', dog.res_similarity(cat, genesis_ic) )\n",
    "print ('' )\n",
    "#print 'Lin Similarity using Brown IC' \n",
    "#print 'hit & slap: ', hit.lin_similarity(slap, brown_ic)     # Lin Similarity \n",
    "#print 'dog & cat: ', dog.lin_similarity(cat, brown_ic) \n",
    "print ('' )\n",
    "print ('Lin Similarity using Genesis IC' )\n",
    "print ('hit & slap: ', hit.lin_similarity(slap, genesis_ic))     # Lin Similarity \n",
    "print ('dog & cat: ', dog.lin_similarity(cat, genesis_ic) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Palmer Similarity\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1cc024ad0ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cat.n.01'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Wu-Palmer Similarity'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'man & boy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# Wu-Palmer Similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'dog & cat: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "hit = wn.synset('man.n.01') \n",
    "slap = wn.synset('boy.n.01') \n",
    "dog = wn.synset('dog.n.01') \n",
    "cat = wn.synset('cat.n.01') \n",
    "print ('Wu-Palmer Similarity' )\n",
    "print ('man & boy: ', wn.wup_similarity(hit, slap))     # Wu-Palmer Similarity \n",
    "print ('dog & cat: ', wn.wup_similarity(dog, cat) )\n",
    "print ('' )\n",
    "print ('Leacock-Chodorow Similarity' )\n",
    "print ('man & boy: ', wn.lch_similarity(hit, slap))     # Leacock-Chodorow Similarity \n",
    "print ('dog & cat: ', wn.lch_similarity(dog, cat) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake_or_real_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))\n",
    "\n",
    "txt=\"a long string of text about him and her\"\n",
    "print filter(lambda w: not w in s,txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name __check_build",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-654e7add9f6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Removes stopwords from a list of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m# process, as it may not be compiled yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name __check_build"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With little fanfare this fall, the New York developer who had planned to build an Islamic community center north of the World Trade Center announced that he would instead use the site for a 70-story tower of luxury condos.\\n\\nThose who had rallied in opposition to the building because of its religious affiliation back in 2010 were exultant. “The importance of the defeat of the Ground Zero Mosque cannot be overstated,” Pamela Geller, president of the American Freedom Defense Initiative, wrote on the website Breitbart in September. “The Ground Zero Mosque became a watershed issue in our effort to raise awareness of and ultimately halt and roll back the advance of Islamic law and Islamic supremacism in America.”\\n\\nIt’s all well and good that so many Republicans have condemned Donald Trump’s reprehensible call for “a total and complete shutdown of Muslims entering the United States.” House Speaker Paul D. Ryan (Wis.) was particularly forceful, calling proper attention to the “many Muslims serving in our armed forces, dying for this country.”\\n\\nWhen he was president, George W. Bush honorably put a lid on right-wing Islamophobia. He regularly praised American Muslims and stressed that the United States needed Muslim allies to fight violent extremism. Once Bush was gone, restraint on his side of politics fell away.\\n\\nThus, Trump’s embrace of a religious test for entry to our country did not come out of nowhere. On the contrary, it simply brought us to the bottom of a slippery slope created by the ongoing exploitation of anti-Muslim feeling for political purposes.\\n\\nYou don’t have to reach far back in time to see why Trump figured he had the ideological space for his Muslim ban. Last month, it was Jeb Bush who introduced the idea of linking the rights of Syrian refugees to their religion. He said he was comfortable granting admission to “people like orphans and people who are clearly not going to be terrorists. Or Christians.” Asked how he’d determine who was Christian, he explained that “you can prove you’re a Christian.”\\n\\nSen. Ted Cruz (Tex.) took a similar view, saying , “There is no meaningful risk of Christians committing acts of terror.”\\n\\nTrump took limits on Muslim access to our country to their logical — if un-American and odious — conclusion. Vice President Biden said that Trump was serving up “a very, very dangerous brew,” but the brew has been steeping for a long time. This is why the “Ground Zero Mosque” episode is so instructive.\\n\\nThe demagoguery began with the labeling of the controversy itself. As PolitiFact pointed out, “the proposed mosque is not at or on Ground Zero. It does not directly abut it or overlook it.” It was “two long blocks” away. And while a mosque was part of the proposed cultural center, the plans also included “a swimming pool, gym and basketball court, a 500-seat auditorium, a restaurant and culinary school, a library and art studios.”\\n\\nThis didn’t stop opponents from going over the top, and Newt Gingrich deserved some kind of award for the most incendiary comment of all. “Nazis,” he said, “don’t have the right to put up a sign next to the Holocaust museum in Washington.”\\n\\nWhen President Obama defended the right of developers to build the project, he was — surprise, surprise — accused of being out of touch, and Republicans were happy to make the Muslim center and Obama’s defense of religious rights an issue in the 2010 campaign.\\n\\n“I think it does speak to the lack of connection between the administration and Washington and folks inside the Beltway and mainstream America,” said Sen. John Cornyn (Tex.), who was then chairman of the committee in charge of electing Republicans to the Senate. Voters, he said, felt they were “being lectured to, not listened to.” Sound familiar?\\n\\nAt the time, John Feehery, the veteran Republican strategist, put his finger on why Republicans were so eager to lambaste Obama’s response to the Ground Zero issue. “This will help drive turnout for the GOP base,” he said.\\n\\nThe Republican establishment is now all upset with Trump, but he is simply the revenge of a Republican base that took its leaders’ pandering — on Islam and a host of other issues — seriously.\\n\\nYou can’t be “just a little” intolerant of Muslims, any more than you can be “just a little” prejudiced against Catholics or Jews. Once the door to bigotry is opened, it is very hard to shut.\\n\\nRead more from E.J. Dionne’s archive, follow him on Twitter or subscribe to his updates on Facebook.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    6335 non-null int64\n",
      "title         6335 non-null object\n",
      "text          6335 non-null object\n",
      "label         6335 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d093750dfa42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\"dd\",\"ddd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [\"the quick brown fox jumped over the lazy\",\" dog the fox the dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = X[\"text\"][20].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-9b54bd8448fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-cc4e6df8bae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = CountVectorizer()\n",
    "v = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\"the quick brown fox jumoed over the kazy dog\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.fit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'back': 34, 'would': 384, 'out': 243, 'admission': 8, 'rallied': 270, 'seriously': 298, 'logical': 206, 'forces': 137, 'access': 4, 'pamela': 247, 'court': 85, 'obama': 230, 'thus': 342, 'wrote': 385, 'cornyn': 83, 'nowhere': 229, 'exploitation': 118, 'praised': 260, 'dangerous': 90, 'updates': 359, 'determine': 96, 'trade': 350, 'us': 361, 'states': 315, 'beltway': 44, 'initiative': 172, 'familiar': 123, 'created': 86, 'developer': 97, 'little': 205, 'have': 155, 'political': 256, 'they': 338, 'violent': 367, 'by': 57, 'he': 156, 'country': 84, 'and': 18, 'embrace': 112, 'auditorium': 30, 'top': 346, 'said': 290, 'facebook': 121, 'gingrich': 142, 'call': 58, 'president': 262, '2010': 0, 'gone': 144, 'trump': 351, 'felt': 129, 'site': 306, 'to': 344, 'has': 154, 'community': 75, 'luxury': 208, 'finger': 132, 'september': 297, 'anti': 20, 'tex': 331, 'awareness': 32, 'lid': 200, 'dying': 108, 'more': 214, 'all': 12, 'very': 363, 'once': 234, 'how': 164, 'use': 362, 'united': 357, 'senate': 296, 'with': 382, 'reach': 272, 'response': 281, 'project': 263, 'announced': 19, 'month': 213, 'also': 14, 'when': 375, 'leaders': 197, 'restraint': 283, 'think': 339, 'the': 334, 'electing': 111, 'plans': 254, 'needed': 221, 'labeling': 192, 'school': 292, 'itself': 186, 'twitter': 353, 'on': 233, 'accused': 5, 'fall': 122, 'particularly': 250, 'had': 150, 'being': 43, 'opposition': 238, 'fight': 130, 'culinary': 88, '70': 2, 'like': 201, 'most': 216, 'for': 135, 'swimming': 325, 'geller': 140, 'clearly': 69, 'planned': 253, 'entering': 113, 'attention': 29, 'his': 159, 'kind': 191, 'story': 318, 'basketball': 37, 'explained': 117, 'committing': 74, 'affiliation': 10, 'slope': 308, 'became': 39, 'far': 125, 'but': 56, 'will': 379, 'purposes': 267, 'ongoing': 235, 'steeping': 316, 'gym': 149, 'wis': 381, 'make': 210, 'extremism': 119, 'long': 207, 'vice': 365, 'charge': 66, 'space': 312, 'advance': 9, 'folks': 133, 'law': 196, 'gop': 146, 'veteran': 364, 'reprehensible': 278, 'comfortable': 71, 'feeling': 127, 'studios': 321, 'opened': 236, 'nazis': 220, 'why': 378, 'speak': 313, 'began': 42, 'art': 25, 'blocks': 48, 'jews': 188, 'subscribe': 322, 'shut': 300, 'politics': 257, 're': 271, 'figured': 131, 'fanfare': 124, 'raise': 269, 'door': 106, 'going': 143, 'acts': 6, 'bottom': 49, 'brought': 52, 'bush': 55, 'was': 369, 'right': 285, 'pointed': 255, 'roll': 288, 'ground': 148, 'ted': 327, 'john': 189, 'ban': 35, 'up': 358, 'christian': 67, '500': 1, 'many': 211, 'introduced': 177, 'base': 36, 'did': 99, 'good': 145, 'center': 64, 'their': 335, 'away': 33, 'it': 184, 'an': 17, 'cannot': 62, 'between': 45, 'because': 40, 'new': 222, 'asked': 27, 'this': 340, 'contrary': 81, 'then': 336, 'drive': 107, 'connection': 80, 'un': 356, 'simply': 305, 'religious': 277, 'museum': 217, 'breitbart': 50, 'christians': 68, 'happy': 152, 'islam': 179, 'if': 167, 'two': 354, 'lambaste': 194, 'donald': 105, 'just': 190, 'those': 341, 'entry': 114, 'america': 15, 'listened': 204, 'demagoguery': 94, 'of': 232, 'abut': 3, 'bigotry': 47, 'overstated': 246, 'wing': 380, 'people': 252, 'surprise': 324, 'prejudiced': 261, 'idea': 165, 'religion': 276, 'armed': 24, 'does': 103, 'orphans': 240, 'while': 376, 'host': 162, 'turnout': 352, 'there': 337, 'some': 310, 'as': 26, 'halt': 151, 'touch': 348, 'so': 309, 'limits': 202, 'stressed': 320, 'issue': 182, 'not': 227, 'upset': 360, 'muslim': 218, 'pandering': 248, 'next': 224, 'fell': 128, 'republicans': 280, 'issues': 183, 'side': 302, 'dionne': 101, 'north': 226, 'overlook': 245, 'episode': 115, 'see': 294, 'is': 178, 'controversy': 82, 'prove': 266, 'condemned': 78, 'directly': 102, 'republican': 279, 'included': 171, 'award': 31, 'odious': 231, 'jeb': 187, 'forceful': 136, 'house': 163, 'ultimately': 355, 'committee': 73, 'zero': 388, 'opponents': 237, 'watershed': 371, 'sen': 295, 'establishment': 116, 'lack': 193, 'now': 228, 'freedom': 138, 'cruz': 87, 'eager': 109, 'last': 195, 'stop': 317, 'you': 387, 'library': 199, 'defense': 93, 'ideological': 166, 'feehery': 126, 'american': 16, 'be': 38, 'speaker': 314, 'proper': 264, 'similar': 304, 'conclusion': 77, 'catholics': 63, 'our': 242, 'exultant': 120, 'follow': 134, 'washington': 370, 'ryan': 289, 'syrian': 326, 'are': 23, 'view': 366, 'part': 249, 'incendiary': 170, 'website': 372, 'time': 343, 'well': 373, 'supremacism': 323, 'other': 241, 'test': 330, 'george': 141, 'inside': 173, 'proposed': 265, 'from': 139, 'total': 347, 'no': 225, 'at': 28, 'intolerant': 176, 'islamophobia': 181, 'help': 157, 'chairman': 65, 'hard': 153, 'brew': 51, 'politifact': 258, 'campaign': 60, 'islamic': 180, 'muslims': 219, 'cultural': 89, 'that': 333, 'its': 185, 'been': 41, 'any': 21, 'sign': 303, 'mosque': 215, 'lectured': 198, 'regularly': 275, 'or': 239, 'took': 345, 'put': 268, 'condos': 79, 'building': 54, 'were': 374, 'world': 383, 'in': 169, 'seat': 293, 'york': 386, 'terrorists': 329, 'slippery': 307, 'allies': 13, 'strategist': 319, 'risk': 287, 'comment': 72, 'refugees': 274, 'honorably': 161, 'who': 377, 'granting': 147, 'defended': 92, 'meaningful': 212, 'sound': 311, 'come': 70, 'calling': 59, 'saying': 291, 'can': 61, 'voters': 368, 'serving': 299, 'linking': 203, 'terror': 328, 'importance': 168, 'over': 244, 'against': 11, 'instead': 174, 'deserved': 95, 'tower': 349, 'newt': 223, 'archive': 22, 'pool': 259, 'restaurant': 282, 'than': 332, 'read': 273, 'holocaust': 160, 'rights': 286, 'administration': 7, 'defeat': 91, 'didn': 100, 'mainstream': 209, 'instructive': 175, 'shutdown': 301, 'effort': 110, 'complete': 76, 'biden': 46, 'build': 53, 'don': 104, 'paul': 251, 'developers': 98, 'him': 158, 'revenge': 284}\n",
      "[ 1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.          1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.          1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.          1.          1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.          1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.          1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.          1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511\n",
      "  1.40546511  1.40546511  1.40546511  1.40546511  1.40546511]\n"
     ]
    }
   ],
   "source": [
    "print(v.vocabulary_)\n",
    "print(v.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = v.transform([s[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 389)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.33333333,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.33333333,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.33333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.33333333,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.33333333,  0.33333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.33333333,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.33333333,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.33333333,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = X['text'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adithyas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_word('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_tokenized_lemmas(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['little',\n",
       " 'fanfare',\n",
       " 'fall',\n",
       " 'new',\n",
       " 'york',\n",
       " 'developer',\n",
       " 'planned',\n",
       " 'build',\n",
       " 'islamic',\n",
       " 'community',\n",
       " 'center',\n",
       " 'north',\n",
       " 'world',\n",
       " 'trade',\n",
       " 'center',\n",
       " 'announced',\n",
       " 'instead',\n",
       " 'use',\n",
       " 'site',\n",
       " '70',\n",
       " 'story',\n",
       " 'tower',\n",
       " 'luxury',\n",
       " 'condo',\n",
       " 'rallied',\n",
       " 'opposition',\n",
       " 'building',\n",
       " 'religious',\n",
       " 'affiliation',\n",
       " '2010',\n",
       " 'exultant',\n",
       " 'importance',\n",
       " 'defeat',\n",
       " 'ground',\n",
       " 'zero',\n",
       " 'mosque',\n",
       " 'overstated',\n",
       " 'pamela',\n",
       " 'geller',\n",
       " 'president',\n",
       " 'american',\n",
       " 'freedom',\n",
       " 'defense',\n",
       " 'initiative',\n",
       " 'wrote',\n",
       " 'website',\n",
       " 'breitbart',\n",
       " 'september',\n",
       " 'ground',\n",
       " 'zero',\n",
       " 'mosque',\n",
       " 'watershed',\n",
       " 'issue',\n",
       " 'effort',\n",
       " 'raise',\n",
       " 'awareness',\n",
       " 'ultimately',\n",
       " 'halt',\n",
       " 'roll',\n",
       " 'advance',\n",
       " 'islamic',\n",
       " 'law',\n",
       " 'islamic',\n",
       " 'supremacism',\n",
       " 'america',\n",
       " 's',\n",
       " 'good',\n",
       " 'republican',\n",
       " 'condemned',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 's',\n",
       " 'reprehensible',\n",
       " 'total',\n",
       " 'complete',\n",
       " 'shutdown',\n",
       " 'muslim',\n",
       " 'entering',\n",
       " 'united',\n",
       " 'state',\n",
       " 'house',\n",
       " 'speaker',\n",
       " 'paul',\n",
       " 'd',\n",
       " 'ryan',\n",
       " 'wi',\n",
       " 'wa',\n",
       " 'particularly',\n",
       " 'forceful',\n",
       " 'calling',\n",
       " 'proper',\n",
       " 'attention',\n",
       " 'muslim',\n",
       " 'serving',\n",
       " 'armed',\n",
       " 'force',\n",
       " 'dying',\n",
       " 'country',\n",
       " 'wa',\n",
       " 'president',\n",
       " 'george',\n",
       " 'w',\n",
       " 'bush',\n",
       " 'honorably',\n",
       " 'lid',\n",
       " 'right',\n",
       " 'wing',\n",
       " 'islamophobia',\n",
       " 'regularly',\n",
       " 'praised',\n",
       " 'american',\n",
       " 'muslim',\n",
       " 'stressed',\n",
       " 'united',\n",
       " 'state',\n",
       " 'needed',\n",
       " 'muslim',\n",
       " 'ally',\n",
       " 'fight',\n",
       " 'violent',\n",
       " 'extremism',\n",
       " 'bush',\n",
       " 'wa',\n",
       " 'gone',\n",
       " 'restraint',\n",
       " 'politics',\n",
       " 'fell',\n",
       " 'away',\n",
       " 'trump',\n",
       " 's',\n",
       " 'embrace',\n",
       " 'religious',\n",
       " 'test',\n",
       " 'entry',\n",
       " 'country',\n",
       " 'did',\n",
       " 'come',\n",
       " 'contrary',\n",
       " 'simply',\n",
       " 'brought',\n",
       " 'u',\n",
       " 'slippery',\n",
       " 'slope',\n",
       " 'created',\n",
       " 'ongoing',\n",
       " 'exploitation',\n",
       " 'anti',\n",
       " 'muslim',\n",
       " 'feeling',\n",
       " 'political',\n",
       " 'purpose',\n",
       " 'don',\n",
       " 't',\n",
       " 'reach',\n",
       " 'far',\n",
       " 'time',\n",
       " 'trump',\n",
       " 'figured',\n",
       " 'ideological',\n",
       " 'space',\n",
       " 'muslim',\n",
       " 'ban',\n",
       " 'month',\n",
       " 'wa',\n",
       " 'jeb',\n",
       " 'bush',\n",
       " 'introduced',\n",
       " 'idea',\n",
       " 'linking',\n",
       " 'right',\n",
       " 'syrian',\n",
       " 'refugee',\n",
       " 'religion',\n",
       " 'said',\n",
       " 'wa',\n",
       " 'comfortable',\n",
       " 'granting',\n",
       " 'admission',\n",
       " 'people',\n",
       " 'like',\n",
       " 'orphan',\n",
       " 'people',\n",
       " 'clearly',\n",
       " 'going',\n",
       " 'terrorist',\n",
       " 'christian',\n",
       " 'asked',\n",
       " 'd',\n",
       " 'determine',\n",
       " 'wa',\n",
       " 'christian',\n",
       " 'explained',\n",
       " 'prove',\n",
       " 'christian',\n",
       " 'sen',\n",
       " 'ted',\n",
       " 'cruz',\n",
       " 'tex',\n",
       " 'took',\n",
       " 'similar',\n",
       " 'view',\n",
       " 'saying',\n",
       " 'meaningful',\n",
       " 'risk',\n",
       " 'christian',\n",
       " 'committing',\n",
       " 'act',\n",
       " 'terror',\n",
       " 'trump',\n",
       " 'took',\n",
       " 'limit',\n",
       " 'muslim',\n",
       " 'access',\n",
       " 'country',\n",
       " 'logical',\n",
       " 'american',\n",
       " 'odious',\n",
       " 'conclusion',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'biden',\n",
       " 'said',\n",
       " 'trump',\n",
       " 'wa',\n",
       " 'serving',\n",
       " 'dangerous',\n",
       " 'brew',\n",
       " 'brew',\n",
       " 'ha',\n",
       " 'steeping',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ground',\n",
       " 'zero',\n",
       " 'mosque',\n",
       " 'episode',\n",
       " 'instructive',\n",
       " 'demagoguery',\n",
       " 'began',\n",
       " 'labeling',\n",
       " 'controversy',\n",
       " 'politifact',\n",
       " 'pointed',\n",
       " 'proposed',\n",
       " 'mosque',\n",
       " 'ground',\n",
       " 'zero',\n",
       " 'doe',\n",
       " 'directly',\n",
       " 'abut',\n",
       " 'overlook',\n",
       " 'wa',\n",
       " 'long',\n",
       " 'block',\n",
       " 'away',\n",
       " 'mosque',\n",
       " 'wa',\n",
       " 'proposed',\n",
       " 'cultural',\n",
       " 'center',\n",
       " 'plan',\n",
       " 'included',\n",
       " 'swimming',\n",
       " 'pool',\n",
       " 'gym',\n",
       " 'basketball',\n",
       " 'court',\n",
       " '500',\n",
       " 'seat',\n",
       " 'auditorium',\n",
       " 'restaurant',\n",
       " 'culinary',\n",
       " 'school',\n",
       " 'library',\n",
       " 'art',\n",
       " 'studio',\n",
       " 'didn',\n",
       " 't',\n",
       " 'stop',\n",
       " 'opponent',\n",
       " 'going',\n",
       " 'newt',\n",
       " 'gingrich',\n",
       " 'deserved',\n",
       " 'kind',\n",
       " 'award',\n",
       " 'incendiary',\n",
       " 'comment',\n",
       " 'nazi',\n",
       " 'said',\n",
       " 'don',\n",
       " 't',\n",
       " 'right',\n",
       " 'sign',\n",
       " 'holocaust',\n",
       " 'museum',\n",
       " 'washington',\n",
       " 'president',\n",
       " 'obama',\n",
       " 'defended',\n",
       " 'right',\n",
       " 'developer',\n",
       " 'build',\n",
       " 'project',\n",
       " 'wa',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'accused',\n",
       " 'touch',\n",
       " 'republican',\n",
       " 'happy',\n",
       " 'make',\n",
       " 'muslim',\n",
       " 'center',\n",
       " 'obama',\n",
       " 's',\n",
       " 'defense',\n",
       " 'religious',\n",
       " 'right',\n",
       " 'issue',\n",
       " '2010',\n",
       " 'campaign',\n",
       " 'think',\n",
       " 'doe',\n",
       " 'speak',\n",
       " 'lack',\n",
       " 'connection',\n",
       " 'administration',\n",
       " 'washington',\n",
       " 'folk',\n",
       " 'inside',\n",
       " 'beltway',\n",
       " 'mainstream',\n",
       " 'america',\n",
       " 'said',\n",
       " 'sen',\n",
       " 'john',\n",
       " 'cornyn',\n",
       " 'tex',\n",
       " 'wa',\n",
       " 'chairman',\n",
       " 'committee',\n",
       " 'charge',\n",
       " 'electing',\n",
       " 'republican',\n",
       " 'senate',\n",
       " 'voter',\n",
       " 'said',\n",
       " 'felt',\n",
       " 'lectured',\n",
       " 'listened',\n",
       " 'sound',\n",
       " 'familiar',\n",
       " 'time',\n",
       " 'john',\n",
       " 'feehery',\n",
       " 'veteran',\n",
       " 'republican',\n",
       " 'strategist',\n",
       " 'finger',\n",
       " 'republican',\n",
       " 'eager',\n",
       " 'lambaste',\n",
       " 'obama',\n",
       " 's',\n",
       " 'response',\n",
       " 'ground',\n",
       " 'zero',\n",
       " 'issue',\n",
       " 'help',\n",
       " 'drive',\n",
       " 'turnout',\n",
       " 'gop',\n",
       " 'base',\n",
       " 'said',\n",
       " 'republican',\n",
       " 'establishment',\n",
       " 'upset',\n",
       " 'trump',\n",
       " 'simply',\n",
       " 'revenge',\n",
       " 'republican',\n",
       " 'base',\n",
       " 'took',\n",
       " 'leader',\n",
       " 'pandering',\n",
       " 'islam',\n",
       " 'host',\n",
       " 'issue',\n",
       " 'seriously',\n",
       " 't',\n",
       " 'just',\n",
       " 'little',\n",
       " 'intolerant',\n",
       " 'muslim',\n",
       " 'just',\n",
       " 'little',\n",
       " 'prejudiced',\n",
       " 'catholic',\n",
       " 'jew',\n",
       " 'door',\n",
       " 'bigotry',\n",
       " 'opened',\n",
       " 'hard',\n",
       " 'shut',\n",
       " 'read',\n",
       " 'e',\n",
       " 'j',\n",
       " 'dionne',\n",
       " 's',\n",
       " 'archive',\n",
       " 'follow',\n",
       " 'twitter',\n",
       " 'subscribe',\n",
       " 'update',\n",
       " 'facebook']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = remove_stopwords(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "t = clean(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-f8bb2b116405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The slippery slope to Trump’s proposed ban on Muslims']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = v.transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ImageColor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-dc1828f2a80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/wordcloud/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .wordcloud import (WordCloud, STOPWORDS, random_color_func,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         get_single_color_func)\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolor_from_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = ['WordCloud', 'STOPWORDS', 'random_color_func',\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ImageColor'"
     ]
    }
   ],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reverence', 'veneration', 'awe', 'fear'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    " \n",
    "for syn in wordnet.synsets(\"awe\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "print(set(synonyms))\n",
    "# print(set(antonyms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
