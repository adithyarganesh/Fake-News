{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "import nltk\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wnl = nltk.WordNetLemmatizer()\n",
    "wnl = WordNetLemmatizer()\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mega_clean(string):\n",
    "    string = clean(string)\n",
    "    string = normalize_word(string)\n",
    "    bits = get_tokenized_lemmas(string)\n",
    "    bits = remove_stopwords(bits)\n",
    "    \n",
    "    lem_vect = []\n",
    "    for word in bits: \n",
    "        lem_vect.append(wnl.lemmatize(word, pos='v'))\n",
    "#     print(lem_vect)\n",
    "    c =  mapping(lem_vect)\n",
    "#     print(\"lalalallalala\" + c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(lem_vect):\n",
    "    v = TfidfVectorizer()\n",
    "    v.fit(lem_vect)\n",
    "    text = v.vocabulary_\n",
    "#     print(text)\n",
    "    sent = []\n",
    "    for i in text.keys():\n",
    "        sent.append(i)\n",
    "        sent.append(' ')\n",
    "    xx = [\"0\"]\n",
    "    xx[0]= ''.join(sent)\n",
    "#     print(\"xooxoxoxox\" + xx[0])\n",
    "    return xx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
    "qq = df[\"text\"][20]\n",
    "arr = re.split('.\\n\\n',qq)\n",
    "# print(arr)\n",
    "sentences = []\n",
    "for string in arr:\n",
    "#     print(string)\n",
    "    x = mega_clean(string)\n",
    "    sentences.append(x)\n",
    "#     print(\"\\n\\n\\nFINALLLL\" + x)\n",
    "# print(arr)\n",
    "# print(\"\\n\\n\\n\")\n",
    "# print(sentences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = TfidfVectorizer()\n",
    "vv.fit(sentences)\n",
    "mapp = vv.vocabulary_\n",
    "vv = vv.transform(sentences)\n",
    "armap = vv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2010': 0,\n",
       " '500': 1,\n",
       " '70': 2,\n",
       " 'abut': 3,\n",
       " 'access': 4,\n",
       " 'accuse': 5,\n",
       " 'act': 6,\n",
       " 'administration': 7,\n",
       " 'admission': 8,\n",
       " 'advance': 9,\n",
       " 'affiliation': 10,\n",
       " 'ally': 11,\n",
       " 'america': 12,\n",
       " 'american': 13,\n",
       " 'announce': 14,\n",
       " 'anti': 15,\n",
       " 'archive': 16,\n",
       " 'arm': 17,\n",
       " 'art': 18,\n",
       " 'ask': 19,\n",
       " 'attention': 20,\n",
       " 'auditorium': 21,\n",
       " 'award': 22,\n",
       " 'awareness': 23,\n",
       " 'away': 24,\n",
       " 'ban': 25,\n",
       " 'base': 26,\n",
       " 'basketball': 27,\n",
       " 'begin': 28,\n",
       " 'beltway': 29,\n",
       " 'biden': 30,\n",
       " 'bigotry': 31,\n",
       " 'block': 32,\n",
       " 'breitbart': 33,\n",
       " 'brew': 34,\n",
       " 'bring': 35,\n",
       " 'build': 36,\n",
       " 'bush': 37,\n",
       " 'call': 38,\n",
       " 'campaign': 39,\n",
       " 'catholic': 40,\n",
       " 'center': 41,\n",
       " 'chairman': 42,\n",
       " 'charge': 43,\n",
       " 'christian': 44,\n",
       " 'clearly': 45,\n",
       " 'come': 46,\n",
       " 'comfortable': 47,\n",
       " 'comment': 48,\n",
       " 'commit': 49,\n",
       " 'committee': 50,\n",
       " 'community': 51,\n",
       " 'complete': 52,\n",
       " 'conclusion': 53,\n",
       " 'condemn': 54,\n",
       " 'condo': 55,\n",
       " 'connection': 56,\n",
       " 'contrary': 57,\n",
       " 'controversy': 58,\n",
       " 'cornyn': 59,\n",
       " 'country': 60,\n",
       " 'court': 61,\n",
       " 'create': 62,\n",
       " 'cruz': 63,\n",
       " 'culinary': 64,\n",
       " 'cultural': 65,\n",
       " 'dangerous': 66,\n",
       " 'defeat': 67,\n",
       " 'defend': 68,\n",
       " 'defense': 69,\n",
       " 'demagoguery': 70,\n",
       " 'deserve': 71,\n",
       " 'determine': 72,\n",
       " 'developer': 73,\n",
       " 'didn': 74,\n",
       " 'die': 75,\n",
       " 'dionne': 76,\n",
       " 'directly': 77,\n",
       " 'do': 78,\n",
       " 'doe': 79,\n",
       " 'don': 80,\n",
       " 'donald': 81,\n",
       " 'door': 82,\n",
       " 'drive': 83,\n",
       " 'eager': 84,\n",
       " 'effort': 85,\n",
       " 'elect': 86,\n",
       " 'embrace': 87,\n",
       " 'enter': 88,\n",
       " 'entry': 89,\n",
       " 'episode': 90,\n",
       " 'establishment': 91,\n",
       " 'explain': 92,\n",
       " 'exploitation': 93,\n",
       " 'extremism': 94,\n",
       " 'exultant': 95,\n",
       " 'facebook': 96,\n",
       " 'fall': 97,\n",
       " 'familiar': 98,\n",
       " 'fanfare': 99,\n",
       " 'far': 100,\n",
       " 'feehery': 101,\n",
       " 'feel': 102,\n",
       " 'fell': 103,\n",
       " 'felt': 104,\n",
       " 'fight': 105,\n",
       " 'figure': 106,\n",
       " 'finger': 107,\n",
       " 'folk': 108,\n",
       " 'follow': 109,\n",
       " 'force': 110,\n",
       " 'forceful': 111,\n",
       " 'freedom': 112,\n",
       " 'geller': 113,\n",
       " 'george': 114,\n",
       " 'gingrich': 115,\n",
       " 'go': 116,\n",
       " 'good': 117,\n",
       " 'gop': 118,\n",
       " 'grant': 119,\n",
       " 'grind': 120,\n",
       " 'gym': 121,\n",
       " 'ha': 122,\n",
       " 'halt': 123,\n",
       " 'happy': 124,\n",
       " 'hard': 125,\n",
       " 'help': 126,\n",
       " 'holocaust': 127,\n",
       " 'honorably': 128,\n",
       " 'host': 129,\n",
       " 'house': 130,\n",
       " 'idea': 131,\n",
       " 'ideological': 132,\n",
       " 'importance': 133,\n",
       " 'incendiary': 134,\n",
       " 'include': 135,\n",
       " 'initiative': 136,\n",
       " 'inside': 137,\n",
       " 'instead': 138,\n",
       " 'instructive': 139,\n",
       " 'intolerant': 140,\n",
       " 'introduce': 141,\n",
       " 'islam': 142,\n",
       " 'islamic': 143,\n",
       " 'islamophobia': 144,\n",
       " 'issue': 145,\n",
       " 'jeb': 146,\n",
       " 'jew': 147,\n",
       " 'john': 148,\n",
       " 'just': 149,\n",
       " 'kind': 150,\n",
       " 'label': 151,\n",
       " 'lack': 152,\n",
       " 'lambaste': 153,\n",
       " 'law': 154,\n",
       " 'leader': 155,\n",
       " 'lecture': 156,\n",
       " 'library': 157,\n",
       " 'lid': 158,\n",
       " 'like': 159,\n",
       " 'limit': 160,\n",
       " 'link': 161,\n",
       " 'listen': 162,\n",
       " 'little': 163,\n",
       " 'logical': 164,\n",
       " 'long': 165,\n",
       " 'luxury': 166,\n",
       " 'mainstream': 167,\n",
       " 'make': 168,\n",
       " 'meaningful': 169,\n",
       " 'month': 170,\n",
       " 'mosque': 171,\n",
       " 'museum': 172,\n",
       " 'muslim': 173,\n",
       " 'nazi': 174,\n",
       " 'need': 175,\n",
       " 'new': 176,\n",
       " 'newt': 177,\n",
       " 'north': 178,\n",
       " 'obama': 179,\n",
       " 'odious': 180,\n",
       " 'ongoing': 181,\n",
       " 'open': 182,\n",
       " 'opponent': 183,\n",
       " 'opposition': 184,\n",
       " 'orphan': 185,\n",
       " 'overlook': 186,\n",
       " 'overstate': 187,\n",
       " 'pamela': 188,\n",
       " 'pander': 189,\n",
       " 'particularly': 190,\n",
       " 'paul': 191,\n",
       " 'people': 192,\n",
       " 'plan': 193,\n",
       " 'point': 194,\n",
       " 'political': 195,\n",
       " 'politics': 196,\n",
       " 'politifact': 197,\n",
       " 'pool': 198,\n",
       " 'praise': 199,\n",
       " 'prejudice': 200,\n",
       " 'president': 201,\n",
       " 'project': 202,\n",
       " 'proper': 203,\n",
       " 'propose': 204,\n",
       " 'prove': 205,\n",
       " 'purpose': 206,\n",
       " 'raise': 207,\n",
       " 'rally': 208,\n",
       " 'reach': 209,\n",
       " 'read': 210,\n",
       " 'refugee': 211,\n",
       " 'regularly': 212,\n",
       " 'religion': 213,\n",
       " 'religious': 214,\n",
       " 'reprehensible': 215,\n",
       " 'republican': 216,\n",
       " 'response': 217,\n",
       " 'restaurant': 218,\n",
       " 'restraint': 219,\n",
       " 'revenge': 220,\n",
       " 'right': 221,\n",
       " 'risk': 222,\n",
       " 'roll': 223,\n",
       " 'ryan': 224,\n",
       " 'say': 225,\n",
       " 'school': 226,\n",
       " 'seat': 227,\n",
       " 'sen': 228,\n",
       " 'senate': 229,\n",
       " 'september': 230,\n",
       " 'seriously': 231,\n",
       " 'serve': 232,\n",
       " 'shut': 233,\n",
       " 'shutdown': 234,\n",
       " 'sign': 235,\n",
       " 'similar': 236,\n",
       " 'simply': 237,\n",
       " 'site': 238,\n",
       " 'slippery': 239,\n",
       " 'slope': 240,\n",
       " 'sound': 241,\n",
       " 'space': 242,\n",
       " 'speak': 243,\n",
       " 'speaker': 244,\n",
       " 'state': 245,\n",
       " 'steep': 246,\n",
       " 'stop': 247,\n",
       " 'story': 248,\n",
       " 'strategist': 249,\n",
       " 'stress': 250,\n",
       " 'studio': 251,\n",
       " 'subscribe': 252,\n",
       " 'supremacism': 253,\n",
       " 'surprise': 254,\n",
       " 'swim': 255,\n",
       " 'syrian': 256,\n",
       " 'take': 257,\n",
       " 'ted': 258,\n",
       " 'terror': 259,\n",
       " 'terrorist': 260,\n",
       " 'test': 261,\n",
       " 'tex': 262,\n",
       " 'think': 263,\n",
       " 'time': 264,\n",
       " 'total': 265,\n",
       " 'touch': 266,\n",
       " 'tower': 267,\n",
       " 'trade': 268,\n",
       " 'trump': 269,\n",
       " 'turnout': 270,\n",
       " 'twitter': 271,\n",
       " 'ultimately': 272,\n",
       " 'unite': 273,\n",
       " 'update': 274,\n",
       " 'upset': 275,\n",
       " 'use': 276,\n",
       " 'veteran': 277,\n",
       " 'vice': 278,\n",
       " 'view': 279,\n",
       " 'violent': 280,\n",
       " 'voter': 281,\n",
       " 'wa': 282,\n",
       " 'washington': 283,\n",
       " 'watershed': 284,\n",
       " 'website': 285,\n",
       " 'wi': 286,\n",
       " 'wing': 287,\n",
       " 'world': 288,\n",
       " 'write': 289,\n",
       " 'york': 290,\n",
       " 'zero': 291}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.21693981, ..., 0.        , 0.21693981,\n",
       "        0.        ],\n",
       "       [0.15283472, 0.        , 0.        , ..., 0.17549585, 0.        ,\n",
       "        0.12428507],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "armap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for i in armap:\n",
    "    sums  = sums + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = np.min(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = {}\n",
    "count = 0\n",
    "for i in sums:\n",
    "    city[count] = i-minimum\n",
    "    count = count+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
